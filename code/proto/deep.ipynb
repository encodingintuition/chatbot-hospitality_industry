{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "python=3.7\n",
    "did pip\n",
    "tensorflow==2.3.1\n",
    "nltk==3.5\n",
    "colorama==0.4.3\n",
    "numpy==1.18.5\n",
    "scikit_learn==0.23.2\n",
    "Flask==1.1.2\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "import json \n",
    "import numpy as np \n",
    "\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open json file\n",
    "with open('intents.json') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Var define\n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "labels = []\n",
    "responses = []\n",
    "\n",
    "# loop to load data \n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        #\n",
    "        training_sentences.append(pattern)\n",
    "        training_labels.append(intent['tag'])\n",
    "    #\n",
    "    responses.append(intent['responses'])\n",
    "\n",
    "    # creates labels list\n",
    "    if intent['tag'] not in labels:\n",
    "        labels.append(intent['tag'])\n",
    "\n",
    "\n",
    "num_classes = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transform Target Labels \n",
    "\n",
    "# define model \n",
    "lbl_encoder = LabelEncoder()\n",
    "# fit model \n",
    "lbl_encoder.fit(training_labels)\n",
    "# transform model \n",
    "training_labels = lbl_encoder.transform(training_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can also add “oov_token” which is a value for “out of token” to deal with out of vocabulary words(tokens) at inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "# Var define\n",
    "vocab_size = 1000\n",
    "embedding_dim = 16\n",
    "max_len = 20 \n",
    "oov_token = '<00V>'\n",
    "\n",
    "# declare tokenizer\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_token)\n",
    "# fit tokenier\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequencees(training_sentences)\n",
    "# creates equal sizes between the text sequences\n",
    "padded_sequences = pad_sequences(sequences, truncating = 'post', maxlen = max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define NN Model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(16, actication='relu'))\n",
    "model.add(Dense(num_classes, activation = 'softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "\n",
    "model.compile(loss = 'sparse_categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Summory \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model \n",
    "\n",
    "epochs = 500\n",
    "history = model.fit(padded_sequences, np.array(training_labels),  epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a Pickle\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained model \n",
    "model.save('chat_model')\n",
    "\n",
    "# save the fitted Tokenizer\n",
    "with open('tokenizer.pickle','wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# to save the fittend label encoder\n",
    "with open('label_encoder.pickle', 'wb') as ecn_file:\n",
    "    pickle.dump(lbl_encoder, ecn_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "491066085bc0e923f6e557547f6ce1485577e225af4b8f4e078d4e99b56857e2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('chatbot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
